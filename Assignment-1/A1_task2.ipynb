{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "input_dir = os.path.join(os.getcwd(), 'data')\n",
    "input_text_file = os.path.join(input_dir, 'corpus.txt')\n",
    "label_file = os.path.join(input_dir, 'labels.txt')\n",
    "\n",
    "with open(input_text_file, 'r') as f:\n",
    "    corpus = f.readlines()\n",
    "    for i in range(len(corpus)):\n",
    "        corpus[i] = corpus[i][:-1]\n",
    "\n",
    "with open(label_file, 'r') as f:\n",
    "    labels = f.readlines()\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = labels[i][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'sadness', 'score': 0.00029859962523914874}\n",
      "{'label': 'joy', 'score': 0.9987986087799072}\n",
      "{'label': 'love', 'score': 0.0004451328422874212}\n",
      "{'label': 'anger', 'score': 0.0001878843759186566}\n",
      "{'label': 'fear', 'score': 0.00012197871546959504}\n",
      "{'label': 'surprise', 'score': 0.00014771465794183314}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khush\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True,)\n",
    "\n",
    "def emotion_scores(sample): \n",
    "    emotion=classifier(sample)\n",
    "    return emotion[0]\n",
    "\n",
    "sample = \"I am so happy to see you!\"\n",
    "all_classes = emotion_scores(sample)\n",
    "for info in all_classes:\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "class BigramLM:\n",
    "    def __init__(self, corpus, labels):\n",
    "        self.corpus = corpus\n",
    "        self.labels = labels\n",
    "        self.bigram_counts = {}\n",
    "        self.unigram_counts = {}\n",
    "        self.vocabulary = set()\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        '''\n",
    "        Train the Bigram language model on the corpus and labels.\n",
    "        '''\n",
    "        self.count_unigrams()\n",
    "        self.count_bigrams()\n",
    "    \n",
    "    \n",
    "    def count_unigrams(self):\n",
    "        '''\n",
    "        Count the unigrams in the corpus and store counts in the unigram_counts dictionary\n",
    "        '''\n",
    "        for i in range(len(self.corpus)):\n",
    "            sentence = self.corpus[i]\n",
    "            tokens = ['</start>'] + sentence.split()\n",
    "            label = self.labels[i]\n",
    "            for token in tokens:\n",
    "                self.unigram_counts[token] = self.unigram_counts.get(token, 0) + 1\n",
    "                self.vocabulary.add(token)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def count_bigrams(self):\n",
    "        '''\n",
    "        Count the bigrams in the corpus and store counts in the bigram_counts dictionary\n",
    "        '''\n",
    "        for i in range(len(self.corpus)):\n",
    "            sentence = self.corpus[i]\n",
    "            tokens = ['</start>'] + sentence.split()\n",
    "            label = self.labels[i]\n",
    "            bi_grams = self.get_bigrams(tokens)            \n",
    "            \n",
    "            for bi_gram in bi_grams:\n",
    "                context = bi_gram[0]\n",
    "                token = bi_gram[1]\n",
    "                if context not in self.bigram_counts:\n",
    "                    self.bigram_counts[context] = {}\n",
    "                self.bigram_counts[context][token] = self.bigram_counts[context].get(token, 0) + 1\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def get_bigrams(self, tokens: List[str]):\n",
    "        '''\n",
    "        Given a list of tokens, return a list of possible bigrams\n",
    "        '''\n",
    "        bigrams = []\n",
    "        for i in range(len(tokens) - 1):\n",
    "            bigrams.append((tokens[i], tokens[i+1]))\n",
    "        return bigrams\n",
    "    \n",
    "    \n",
    "    def get_bigram_prob(self, context: str, token: str, smoothing: str = 'none'):\n",
    "        '''\n",
    "        Get the probability of the token given the context\n",
    "        '''\n",
    "        smoothing = smoothing.lower()\n",
    "        if smoothing == 'none':\n",
    "            return self.__get_bigram_prob_normal(context, token)\n",
    "        elif smoothing == 'laplace':\n",
    "            return self.__get_bigram_prob_laplace(context, token)\n",
    "        elif smoothing == 'kneser-ney':\n",
    "            return self.__get_bigram_prob_kneser_ney(context, token)\n",
    "        else:\n",
    "            raise ValueError('Smoothing method not supported')\n",
    "    \n",
    "    \n",
    "    def __get_bigram_prob_normal(self, context, token):\n",
    "        context_token_cnt = self.bigram_counts[context].get(token, 0)\n",
    "        return context_token_cnt / self.unigram_counts[context]\n",
    "\n",
    "\n",
    "    def __get_bigram_prob_laplace(self, context, token):\n",
    "        context_token_cnt = self.bigram_counts[context].get(token, 0)\n",
    "        return (context_token_cnt + 1) / (self.unigram_counts[context] + len(self.vocabulary))\n",
    "    \n",
    "    \n",
    "    def __get_bigram_prob_kneser_ney(self, context, token, avg_discount=0.7):\n",
    "        d = avg_discount\n",
    "        context_token_cnt = self.bigram_counts[context].get(token, 0)\n",
    "        \n",
    "        # Calculate alpha, which depends on the context\n",
    "        alpha = d * len(self.bigram_counts[context]) / self.unigram_counts[context]\n",
    "        # Calculate Continuation Probability, which depends on the token\n",
    "        bigram_with_token_cnt = 0\n",
    "        for _context_ in self.bigram_counts:\n",
    "            bigram_with_token_cnt += 1 if token in self.bigram_counts[_context_] else 0\n",
    "        total_biagram_pairs = 0\n",
    "        for _context_ in self.bigram_counts:\n",
    "            total_biagram_pairs += len(self.bigram_counts[_context_])\n",
    "        P_continuation = bigram_with_token_cnt / total_biagram_pairs\n",
    "        \n",
    "        return (max(context_token_cnt - d, 0) / self.unigram_counts[context]) + (alpha * P_continuation)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_lm = BigramLM(corpus, labels)\n",
    "bigram_lm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3789"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_lm.unigram_counts['i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0771809335374076"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_lm.get_bigram_prob('i', 'am', smoothing='kneser-ney')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3789"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_lm.unigram_counts['i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "context = {'word': ['i', 'am', 'happy'], 'prob': [0.1, 0.2, 0.7]}\n",
    "random.choices(context['word'], context['prob'])[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
