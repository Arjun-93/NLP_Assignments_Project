{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), 'data')\n",
    "input_text_file = os.path.join(input_dir, 'corpus.txt')\n",
    "label_file = os.path.join(input_dir, 'labels.txt')\n",
    "\n",
    "with open(input_text_file, 'r') as f:\n",
    "    corpus = f.readlines()\n",
    "    for i in range(len(corpus)):\n",
    "        corpus[i] = corpus[i][:-1]\n",
    "\n",
    "with open(label_file, 'r') as f:\n",
    "    labels = f.readlines()\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = labels[i][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 2400)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus), len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\khush\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "{'label': 'sadness', 'score': 0.00029859962523914874}\n",
      "{'label': 'joy', 'score': 0.9987986087799072}\n",
      "{'label': 'love', 'score': 0.0004451328422874212}\n",
      "{'label': 'anger', 'score': 0.0001878843759186566}\n",
      "{'label': 'fear', 'score': 0.00012197871546959504}\n",
      "{'label': 'surprise', 'score': 0.00014771465794183314}\n",
      "<class 'list'>\n",
      "Time taken for emotion_scores:  0.8889884948730469\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "classifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True,)\n",
    "\n",
    "def emotion_scores(sample): \n",
    "    emotion=classifier(sample)\n",
    "    return emotion[0]\n",
    "\n",
    "start_time = time.time()\n",
    "sample = \"I am so happy to see you!\"\n",
    "all_classes = emotion_scores(sample)\n",
    "for info in all_classes:\n",
    "    print(info)\n",
    "end_time = time.time()\n",
    "print(type(all_classes))\n",
    "print(\"Time taken for emotion_scores: \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Model for Emotion Sentence Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class BigramLM_Emotion:\n",
    "    def __init__(self, corpus, labels):\n",
    "        self.corpus = corpus\n",
    "        self.labels = labels\n",
    "        self.bigram_counts = {}\n",
    "        self.unigram_counts = {}\n",
    "        self.vocabulary = set()\n",
    "        self.total_bigram_pairs = None\n",
    "        self.bigram_emotion_vector = {}\n",
    "        self.bigram_prob = {}\n",
    "        self.class_to_idx = {'sadness': 0, 'joy': 1, 'love': 2, 'anger': 3, 'fear': 4, 'surprise': 5}\n",
    "        self.num_labels = len(self.class_to_idx)\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        '''\n",
    "        Train the Bigram language model on the corpus and labels.\n",
    "        '''\n",
    "        self.count_unigrams()\n",
    "        self.count_bigrams()\n",
    "        self.__make_emotion_vector_to_numpy()\n",
    "    \n",
    "    \n",
    "    def count_unigrams(self):\n",
    "        '''\n",
    "        Count the unigrams in the corpus and store counts in the unigram_counts dictionary\n",
    "        '''\n",
    "        for i in range(len(self.corpus)):\n",
    "            sentence = self.corpus[i]\n",
    "            tokens = ['</start>'] + sentence.split() + ['</end>']\n",
    "            label = self.labels[i]\n",
    "            for token in tokens:\n",
    "                self.unigram_counts[token] = self.unigram_counts.get(token, 0) + 1\n",
    "                self.vocabulary.add(token)\n",
    "        self.vocabulary.remove('</start>')\n",
    "        self.vocabulary.remove('</end>')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def count_bigrams(self):\n",
    "        '''\n",
    "        Count the bigrams in the corpus and store counts in the bigram_counts dictionary\n",
    "        '''\n",
    "        for i in range(len(self.corpus)):\n",
    "            if i % 100 == 0:\n",
    "                print(i, end=' ')\n",
    "            sentence = self.corpus[i]\n",
    "            emotion = emotion_scores(sentence)\n",
    "            label = self.labels[i]\n",
    "            tokens = ['</start>'] + sentence.split() + ['</end>']\n",
    "            bi_grams = self.get_bigrams(tokens)            \n",
    "            \n",
    "            for bi_gram in bi_grams:\n",
    "                context = bi_gram[0]\n",
    "                token = bi_gram[1]\n",
    "                if context not in self.bigram_counts:\n",
    "                    self.bigram_counts[context] = {}\n",
    "                self.bigram_counts[context][token] = self.bigram_counts[context].get(token, 0) + 1\n",
    "                if context not in self.bigram_emotion_vector:\n",
    "                    self.bigram_emotion_vector[context] = {}\n",
    "                self.bigram_emotion_vector[context][token] = self.bigram_emotion_vector[context].get(token, [])\n",
    "                emotion_vector = [0] * self.num_labels\n",
    "                for i in range(len(emotion)):\n",
    "                    emotion_vector[self.class_to_idx[emotion[i]['label']]] = emotion[i]['score']\n",
    "                self.bigram_emotion_vector[context][token].append(emotion_vector)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def get_bigrams(self, tokens:List[str]):\n",
    "        '''\n",
    "        Given a list of tokens, return a list of possible bigrams\n",
    "        '''\n",
    "        bigrams = []\n",
    "        for i in range(len(tokens) - 1):\n",
    "            bigrams.append((tokens[i], tokens[i+1]))\n",
    "        return bigrams\n",
    "    \n",
    "    \n",
    "    def __make_emotion_vector_to_numpy(self):\n",
    "        '''\n",
    "        Convert emotion vectors to numpy arrays\n",
    "        '''\n",
    "        for context in self.bigram_emotion_vector:\n",
    "            for token in self.bigram_emotion_vector[context]:\n",
    "                self.bigram_emotion_vector[context][token] = np.array(self.bigram_emotion_vector[context][token])\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def get_bigram_prob(self, context:str, token:str, beta_score:float, smoothing:str='kneser-ney'):\n",
    "        '''\n",
    "        Get the probability of the token given the context\n",
    "        '''\n",
    "        smoothing = smoothing.lower()\n",
    "        if smoothing == 'none':\n",
    "            return self.__get_bigram_prob_normal(context, token, beta_score)\n",
    "        elif smoothing == 'laplace':\n",
    "            return self.__get_bigram_prob_laplace(context, token, beta_score)\n",
    "        elif smoothing == 'kneser-ney':\n",
    "            return self.__get_bigram_prob_kneser_ney(context, token, beta_score)\n",
    "        else:\n",
    "            raise ValueError('Smoothing method not supported')\n",
    "    \n",
    "    \n",
    "    def __get_bigram_prob_normal(self, context, token, beta_score):\n",
    "        context_token_cnt = self.bigram_counts[context].get(token, 0)\n",
    "        return beta_score + beta_score*context_token_cnt/self.unigram_counts[context]\n",
    "\n",
    "\n",
    "    def __get_bigram_prob_laplace(self, context, token, beta_score):\n",
    "        context_token_cnt = self.bigram_counts[context].get(token, 0)\n",
    "        return beta_score + beta_score*(context_token_cnt+1)/(self.unigram_counts[context]+len(self.vocabulary))\n",
    "    \n",
    "    \n",
    "    def __get_bigram_prob_kneser_ney(self, context, token, beta_score, avg_discount=0.7):\n",
    "        d = avg_discount\n",
    "        context_token_cnt = self.bigram_counts[context].get(token, 0)\n",
    "        \n",
    "        # Calculate alpha, which depends on the context\n",
    "        alpha = d * len(self.bigram_counts[context]) / self.unigram_counts[context]\n",
    "        # Calculate Continuation Probability, which depends on the token\n",
    "        bigram_with_token_cnt = 0\n",
    "        for _context_ in self.bigram_counts:\n",
    "            bigram_with_token_cnt += 1 if token in self.bigram_counts[_context_] else 0\n",
    "        total_bigram_pairs = self.__count_total_bigram_pairs()\n",
    "        P_continuation = bigram_with_token_cnt / total_bigram_pairs\n",
    "    \n",
    "        return (beta_score + beta_score*max(context_token_cnt-d,0)/self.unigram_counts[context]) + (alpha * P_continuation)\n",
    "    \n",
    "    \n",
    "    def __count_total_bigram_pairs(self):\n",
    "        '''\n",
    "        Count the total number of unique bigram pairs in the corpus\n",
    "        '''\n",
    "        if self.total_bigram_pairs == None:\n",
    "            self.total_bigram_pairs = 0\n",
    "            for context in self.bigram_counts:\n",
    "                self.total_bigram_pairs += len(self.bigram_counts[context])\n",
    "        return self.total_bigram_pairs\n",
    "    \n",
    "    \n",
    "    def __generate_bigram_prob_for_context(self, context, emotion:str, smoothing:str='kneser-ney'):\n",
    "        '''\n",
    "        Generate bigram probabilities for all tokens for a given context\n",
    "        '''\n",
    "        if context not in self.bigram_prob:\n",
    "            self.bigram_prob[context] = {}\n",
    "        if emotion not in self.bigram_prob[context]:\n",
    "            self.bigram_prob[context][emotion] = {}\n",
    "            emotion_vector = np.array([0] * self.num_labels)\n",
    "            emotion_vector[self.class_to_idx[emotion]] = 1\n",
    "\n",
    "            # calculate beta scores and normalize them\n",
    "            beta_scores = {}\n",
    "            total_score = 0\n",
    "            for token in self.bigram_counts[context]:\n",
    "                score = np.sum(emotion_vector * self.bigram_emotion_vector[context][token])\n",
    "                # Best score combinations\n",
    "                # score * np.log(score), score * (1 + np.log(score)), score * (np.e + np.log(score)) \n",
    "                beta_scores[token] =  score * np.log(score)\n",
    "                total_score += score\n",
    "            for token in beta_scores:\n",
    "                beta_scores[token] = beta_scores[token] / total_score\n",
    "                \n",
    "            # calculate bigram probabilities\n",
    "            for token in self.bigram_counts[context]:\n",
    "                self.bigram_prob[context][emotion][token] = self.get_bigram_prob(context, token, beta_scores[token], smoothing)\n",
    "            \n",
    "            # normalize bigram probabilities\n",
    "            total_prob = sum(self.bigram_prob[context][emotion].values())\n",
    "            for token in self.bigram_prob[context][emotion]:\n",
    "                self.bigram_prob[context][emotion][token] = self.bigram_prob[context][emotion][token] / total_prob\n",
    "\n",
    "        return self.bigram_prob[context][emotion]\n",
    "    \n",
    "    \n",
    "    def __generate_token(self, context:str, emotion:str, smoothing:str='kneser-ney'):\n",
    "        '''\n",
    "        Generate a token given the context\n",
    "        '''\n",
    "        all_possible_tokens = self.__generate_bigram_prob_for_context(context, emotion, smoothing)\n",
    "        generated_token = random.choices(list(all_possible_tokens.keys()), weights=list(all_possible_tokens.values()))[0]\n",
    "        return generated_token\n",
    "    \n",
    "    \n",
    "    def generate_sentence(self, emotion:str, max_length:int=10, smoothing:str='kneser-ney'):\n",
    "        '''\n",
    "        Generate a sentence of the given max_length\n",
    "        '''\n",
    "        sentence = []\n",
    "        context = '</start>'\n",
    "        for _ in range(max_length):\n",
    "            token = self.__generate_token(context, emotion, smoothing)\n",
    "            if token == '</end>':\n",
    "                break\n",
    "            sentence.append(token)\n",
    "            context = token\n",
    "        return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 "
     ]
    }
   ],
   "source": [
    "lm = BigramLM_Emotion(corpus, labels)\n",
    "lm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sadness': 0, 'joy': 1, 'love': 2, 'anger': 3, 'fear': 4, 'surprise': 5}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_file = os.path.join(os.getcwd(), 'bigram_lm_emotion.pkl')\n",
    "pickle.dump(lm, open(model_file, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "model_file = os.path.join(os.getcwd(), 'bigram_lm_emotion.pkl')\n",
    "lm_saved = pickle.load(open(model_file, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate different emotion sentences using the LM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sentences for sadness\n",
      "Generated 50 sentences for sadness in 73 trails\n",
      "Accuracy for sadness: 68.4931506849315\n",
      "\n",
      "Generating sentences for joy\n",
      "Generated 50 sentences for joy in 58 trails\n",
      "Accuracy for joy: 86.20689655172413\n",
      "\n",
      "Generating sentences for love\n",
      "Generated 50 sentences for love in 90 trails\n",
      "Accuracy for love: 55.55555555555556\n",
      "\n",
      "Generating sentences for anger\n",
      "Generated 50 sentences for anger in 69 trails\n",
      "Accuracy for anger: 72.46376811594203\n",
      "\n",
      "Generating sentences for fear\n",
      "Generated 50 sentences for fear in 68 trails\n",
      "Accuracy for fear: 73.52941176470588\n",
      "\n",
      "Generating sentences for surprise\n",
      "Generated 50 sentences for surprise in 71 trails\n",
      "Accuracy for surprise: 70.4225352112676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_emotions = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "output_dir = os.path.join(os.getcwd(), 'output')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "for emotion in all_emotions:\n",
    "    print(f'Generating sentences for {emotion}')\n",
    "    output_file = os.path.join(output_dir, f'gen_{emotion}.txt')\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        sentences = []\n",
    "        outputs = []\n",
    "        samples = 50\n",
    "        generated = 0\n",
    "        trails = 0\n",
    "        while generated < samples:\n",
    "            sentence = lm.generate_sentence(emotion, max_length=30, smoothing='kneser-ney')\n",
    "            emotions = emotion_scores(sentence)\n",
    "            trails += 1\n",
    "            \n",
    "            max_score = 0\n",
    "            max_label = ''\n",
    "            for info in emotions:\n",
    "                if info['score'] > max_score:\n",
    "                    max_score = info['score']\n",
    "                    max_label = info['label']\n",
    "            if max_label != emotion:\n",
    "                continue\n",
    "            \n",
    "            f.write(sentence + '\\n')\n",
    "            f.write(max_label + ' ' + str(max_score) + '\\n\\n')\n",
    "            sentences.append(sentence)\n",
    "            outputs.append(emotions)\n",
    "            generated += 1\n",
    "        \n",
    "    print(f'Generated {generated} sentences for {emotion} in {trails} trails')\n",
    "    print(f'Accuracy for {emotion}: {generated/trails*100}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
