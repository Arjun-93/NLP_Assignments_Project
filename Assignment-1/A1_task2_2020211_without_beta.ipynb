{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), 'data')\n",
    "input_text_file = os.path.join(input_dir, 'corpus.txt')\n",
    "label_file = os.path.join(input_dir, 'labels.txt')\n",
    "\n",
    "with open(input_text_file, 'r') as f:\n",
    "    corpus = f.readlines()\n",
    "    for i in range(len(corpus)):\n",
    "        corpus[i] = corpus[i][:-1]\n",
    "\n",
    "with open(label_file, 'r') as f:\n",
    "    labels = f.readlines()\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = labels[i][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 2400)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus), len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\khush\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "{'label': 'sadness', 'score': 0.00029859962523914874}\n",
      "{'label': 'joy', 'score': 0.9987986087799072}\n",
      "{'label': 'love', 'score': 0.0004451328422874212}\n",
      "{'label': 'anger', 'score': 0.0001878843759186566}\n",
      "{'label': 'fear', 'score': 0.00012197871546959504}\n",
      "{'label': 'surprise', 'score': 0.00014771465794183314}\n",
      "<class 'list'>\n",
      "Time taken for emotion_scores:  0.8111534118652344\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "classifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True,)\n",
    "\n",
    "def emotion_scores(sample): \n",
    "    emotion=classifier(sample)\n",
    "    return emotion[0]\n",
    "\n",
    "start_time = time.time()\n",
    "sample = \"I am so happy to see you!\"\n",
    "all_classes = emotion_scores(sample)\n",
    "for info in all_classes:\n",
    "    print(info)\n",
    "end_time = time.time()\n",
    "print(type(all_classes))\n",
    "print(\"Time taken for emotion_scores: \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "import random\n",
    "\n",
    "\n",
    "class BigramLM:\n",
    "    def __init__(self, corpus, labels):\n",
    "        self.corpus = corpus\n",
    "        self.labels = labels\n",
    "        self.bigram_counts = {}\n",
    "        self.unigram_counts = {}\n",
    "        self.vocabulary = set()\n",
    "        self.total_bigram_pairs = None\n",
    "        self.bigram_prob = {}\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        '''\n",
    "        Train the Bigram language model on the corpus and labels.\n",
    "        '''\n",
    "        self.count_unigrams()\n",
    "        self.count_bigrams()\n",
    "    \n",
    "    \n",
    "    def count_unigrams(self):\n",
    "        '''\n",
    "        Count the unigrams in the corpus and store counts in the unigram_counts dictionary\n",
    "        '''\n",
    "        for i in range(len(self.corpus)):\n",
    "            sentence = self.corpus[i]\n",
    "            tokens = ['</start>'] + sentence.split() + ['</end>']\n",
    "            label = self.labels[i]\n",
    "            for token in tokens:\n",
    "                self.unigram_counts[token] = self.unigram_counts.get(token, 0) + 1\n",
    "                self.vocabulary.add(token)\n",
    "        self.vocabulary.remove('</start>')\n",
    "        self.vocabulary.remove('</end>')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def count_bigrams(self):\n",
    "        '''\n",
    "        Count the bigrams in the corpus and store counts in the bigram_counts dictionary\n",
    "        '''\n",
    "        for i in range(len(self.corpus)):\n",
    "            sentence = self.corpus[i]\n",
    "            tokens = ['</start>'] + sentence.split() + ['</end>']\n",
    "            label = self.labels[i]\n",
    "            bi_grams = self.get_bigrams(tokens)            \n",
    "            \n",
    "            for bi_gram in bi_grams:\n",
    "                context = bi_gram[0]\n",
    "                token = bi_gram[1]\n",
    "                if context not in self.bigram_counts:\n",
    "                    self.bigram_counts[context] = {}\n",
    "                self.bigram_counts[context][token] = self.bigram_counts[context].get(token, 0) + 1\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def get_bigrams(self, tokens: List[str]):\n",
    "        '''\n",
    "        Given a list of tokens, return a list of possible bigrams\n",
    "        '''\n",
    "        bigrams = []\n",
    "        for i in range(len(tokens) - 1):\n",
    "            bigrams.append((tokens[i], tokens[i+1]))\n",
    "        return bigrams\n",
    "    \n",
    "    \n",
    "    def get_bigram_prob(self, context: str, token: str, smoothing: str = 'none'):\n",
    "        '''\n",
    "        Get the probability of the token given the context\n",
    "        '''\n",
    "        smoothing = smoothing.lower()\n",
    "        if smoothing == 'none':\n",
    "            return self.__get_bigram_prob_normal(context, token)\n",
    "        elif smoothing == 'laplace':\n",
    "            return self.__get_bigram_prob_laplace(context, token)\n",
    "        elif smoothing == 'kneser-ney':\n",
    "            return self.__get_bigram_prob_kneser_ney(context, token)\n",
    "        else:\n",
    "            raise ValueError('Smoothing method not supported')\n",
    "    \n",
    "    \n",
    "    def __get_bigram_prob_normal(self, context, token):\n",
    "        context_token_cnt = self.bigram_counts[context].get(token, 0)\n",
    "        return context_token_cnt / self.unigram_counts[context]\n",
    "\n",
    "\n",
    "    def __get_bigram_prob_laplace(self, context, token):\n",
    "        context_token_cnt = self.bigram_counts[context].get(token, 0)\n",
    "        return (context_token_cnt + 1) / (self.unigram_counts[context] + len(self.vocabulary))\n",
    "    \n",
    "    \n",
    "    def __get_bigram_prob_kneser_ney(self, context, token, avg_discount=0.7):\n",
    "        d = avg_discount\n",
    "        context_token_cnt = self.bigram_counts[context].get(token, 0)\n",
    "        \n",
    "        # Calculate alpha, which depends on the context\n",
    "        alpha = d * len(self.bigram_counts[context]) / self.unigram_counts[context]\n",
    "        # Calculate Continuation Probability, which depends on the token\n",
    "        bigram_with_token_cnt = 0\n",
    "        for _context_ in self.bigram_counts:\n",
    "            bigram_with_token_cnt += 1 if token in self.bigram_counts[_context_] else 0\n",
    "        total_bigram_pairs = self.__count_total_bigram_pairs()\n",
    "        P_continuation = bigram_with_token_cnt / total_bigram_pairs\n",
    "    \n",
    "        return (max(context_token_cnt - d, 0) / self.unigram_counts[context]) + (alpha * P_continuation)\n",
    "    \n",
    "    \n",
    "    def __count_total_bigram_pairs(self):\n",
    "        '''\n",
    "        Count the total number of unaiue bigram pairs in the corpus\n",
    "        '''\n",
    "        if self.total_bigram_pairs == None:\n",
    "            self.total_bigram_pairs = 0\n",
    "            for context in self.bigram_counts:\n",
    "                self.total_bigram_pairs += len(self.bigram_counts[context])\n",
    "        return self.total_bigram_pairs\n",
    "    \n",
    "    \n",
    "    def __generate_bigram_prob_for_context(self, context, smoothing:str = 'none'):\n",
    "        '''\n",
    "        Generate bigram probabilities for all tokens for a given context\n",
    "        '''\n",
    "        if context not in self.bigram_prob:\n",
    "            self.bigram_prob[context] = {}\n",
    "            for token in self.bigram_counts[context]:\n",
    "                self.bigram_prob[context][token] = self.get_bigram_prob(context, token, smoothing)\n",
    "        return self.bigram_prob[context]\n",
    "    \n",
    "    \n",
    "    def __generate_token(self, context:str, smoothing:str = 'none'):\n",
    "        '''\n",
    "        Generate a token given the context\n",
    "        '''\n",
    "        all_possible_tokens = self.__generate_bigram_prob_for_context(context, smoothing)\n",
    "        generated_token = random.choices(list(all_possible_tokens.keys()), weights=list(all_possible_tokens.values()), k=1)[0]\n",
    "        return generated_token\n",
    "    \n",
    "    \n",
    "    def generate_sentence(self, max_length: int = 10, smoothing: str = 'none'):\n",
    "        '''\n",
    "        Generate a sentence of the given max_length\n",
    "        '''\n",
    "        sentence = []\n",
    "        context = '</start>'\n",
    "        for _ in range(max_length):\n",
    "            token = self.__generate_token(context, smoothing)\n",
    "            if token == '</end>':\n",
    "                break\n",
    "            sentence.append(token)\n",
    "            context = token\n",
    "        return ' '.join(sentence)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_lm = BigramLM(corpus, labels)\n",
    "bigram_lm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00010848340203948796"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_lm.get_bigram_prob('i', 'fine', smoothing='laplace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3789"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_lm.unigram_counts['i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am trying to hold me to me and physical'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_lm.generate_sentence(max_length=10, smoothing='kneser-ney')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating top 5 Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 bigrams with smoothing method: none\n",
      "Total bigrams: 25681\n",
      "('href', 'http'): 1.0\n",
      "('mooshilu', '</end>'): 1.0\n",
      "('tychelle', 'to'): 1.0\n",
      "('hang', 'out'): 1.0\n",
      "('nonexistent', 'social'): 1.0\n",
      "('alex', 'and'): 1.0\n",
      "('marriage', 'and'): 1.0\n",
      "('personifying', 'an'): 1.0\n",
      "('progeny', 'who'): 1.0\n",
      "('genuflecting', 'at'): 1.0\n",
      "\n",
      "Top 10 bigrams with smoothing method: laplace\n",
      "Total bigrams: 25681\n",
      "('</start>', 'i'): 0.2693830629710052\n",
      "('i', 'feel'): 0.11043610327619874\n",
      "('feel', 'like'): 0.0350976507217662\n",
      "('i', 'am'): 0.03189412019960946\n",
      "('</start>', 'im'): 0.02720653978796781\n",
      "('that', 'i'): 0.02650602409638554\n",
      "('and', 'i'): 0.023103748910200523\n",
      "('im', 'feeling'): 0.022454576619814877\n",
      "('i', 'was'): 0.021913647211976566\n",
      "('to', 'be'): 0.01861427094105481\n",
      "\n",
      "Top 10 bigrams with smoothing method: kneser-ney\n",
      "Total bigrams: 25681\n",
      "('href', 'http'): 0.9720021806004439\n",
      "('don', 't'): 0.9712049203427449\n",
      "('didn', 't'): 0.9611413972283877\n",
      "('sort', 'of'): 0.9594087640897253\n",
      "('supposed', 'to'): 0.9238243578261491\n",
      "('doesn', 't'): 0.9222827944567753\n",
      "('www', '</end>'): 0.9166874342899419\n",
      "('amount', 'of'): 0.9137436236906662\n",
      "('wasn', 't'): 0.9125681437638721\n",
      "('able', 'to'): 0.9071576911594824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoothing_methods = ['none', 'laplace', 'kneser-ney']\n",
    "\n",
    "for smoothing in smoothing_methods:\n",
    "    all_bigram = {}\n",
    "    for context in bigram_lm.bigram_counts:\n",
    "        for token in bigram_lm.bigram_counts[context]:\n",
    "            all_bigram[(context, token)] = bigram_lm.get_bigram_prob(context, token, smoothing)\n",
    "    \n",
    "    top_count = 10\n",
    "    top_bigrams = sorted(all_bigram, key=all_bigram.get, reverse=True)[:top_count]\n",
    "    print(f\"Top {top_count} bigrams with smoothing method: {smoothing}\")\n",
    "    print(f\"Total bigrams: {len(all_bigram)}\")\n",
    "    for bigram in top_bigrams:\n",
    "        print(f\"{bigram}: {all_bigram[bigram]}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
