{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBYe3XHfZvL5"
      },
      "source": [
        "# Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR9GYKLmayJ1"
      },
      "source": [
        " Task Definition - Given two sentences, calculate the similarity between these two sentences. The similarity is given as a score ranging from 0to5.\n",
        "<br>\n",
        "\n",
        "Train datapoint examples -\n",
        "<br>\n",
        "</t>score sentence1 sentence2\n",
        "* 4.750 A young child is riding a horse. A child is riding a horse.\n",
        "* 2.400 A woman is playing the guitar. A man is playing guitar.\n",
        "\n",
        "<br>\n",
        "The dataset is already divided into training and validation sets in the files - ‘train.csv’ and ‘dev.csv’, respectively. Both files are given to you in the zip file attached to the assignment. Please note that it is tab-separated. A testing file excluding the score field will be provided to\n",
        "you during the demo to run inference on. You are required to create dataset classes and data loaders appropriately for your training and evaluation setups.\n",
        "For this task, you are required to implement three setups:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNLKKp2ZZTqY"
      },
      "source": [
        "## Setup - 1A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y10QgGjTZwwj"
      },
      "source": [
        "You are required to train a BERT model (google-bert/bert-base-uncased ·\n",
        "Hugging Face) using HuggingFace for the task of Text Similarity. You are required to\n",
        "obtain BERT embeddings while making use of a special token used by BERT for\n",
        "separating multiple sentences in an input text and an appropriate linear layer or setting\n",
        "of BertForSequenceClassification (BERT) framework for a float output. Choose a\n",
        "suitable loss function. Report the required evaluation metric on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYeNbYNXA-F4",
        "outputId": "81f0f87e-0d49-419b-b708-ae346888e64a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wsUpygFZX1r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('/content/train.csv', sep='\\t')\n",
        "\n",
        "# Convert DataFrame to Dataset format compatible with datasets library\n",
        "dataset_dict = {\n",
        "    \"sentence1\": df[\"sentence1\"].tolist(),\n",
        "    \"sentence2\": df[\"sentence2\"].tolist(),\n",
        "    \"label\": df[\"score\"].tolist(),\n",
        "}\n",
        "dataset = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "# Load tokenizer\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Define function to tokenize examples\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "# Tokenize dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Define data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrlN1-wBZXzC"
      },
      "outputs": [],
      "source": [
        "|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-XQGJlvZXwX"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
        "training_args = TrainingArguments(\"test-trainer\")\n",
        "model = BertForSequenceClassification.from_pretrained(checkpoint, num_labels=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EK2LPc91_wZd"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgX0rnACZYS1"
      },
      "source": [
        "## Setup - 1B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhqE2PMCavsu"
      },
      "source": [
        "You are required to make use of the Sentence-BERT model\n",
        "(https://arxiv.org/pdf/1908.10084.pdf) and the SentenceTransformers framework\n",
        "(Sentence-Transformers). For this setup, make use of the Sentence-BERT model to\n",
        "encode the sentences and determine the cosine similarity between these embeddings\n",
        "for the validation set. Report the required evaluation metric on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bs0wL8dxWEOC"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHQSIlCaWTSg"
      },
      "outputs": [],
      "source": [
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siAvNrxkaCLk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "train_df = pd.read_csv('./A3_task1_data_files/train.csv', sep='\\t')\n",
        "val_df = pd.read_csv('./A3_task1_data_files/dev.csv', sep='\\t')\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "train_df['score'] = scaler.fit_transform(train_df[['score']])\n",
        "val_df['score'] = scaler.fit_transform(val_df[['score']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpZOXCj6aId1"
      },
      "outputs": [],
      "source": [
        "# train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZF3N_tGWEKx",
        "outputId": "1ea82bf4-d90d-4f0f-d5e7-74f9a729d217"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correlation coefficient (Pearson correlation) between predicted similarities and actual scores: 0.6379508453621849\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\")\n",
        "val_embeddings = []\n",
        "for _, row in val_df.iterrows():\n",
        "    if pd.notnull(row['sentence1']) and pd.notnull(row['sentence2']):  # Check for missing values\n",
        "        sentence1_embedding = model.encode(row['sentence1'], convert_to_tensor=True)\n",
        "        sentence2_embedding = model.encode(row['sentence2'], convert_to_tensor=True)\n",
        "        val_embeddings.append((sentence1_embedding, sentence2_embedding))\n",
        "\n",
        "# Calculate cosine similarity between embeddings\n",
        "cosine_similarities = []\n",
        "for embedding_pair in val_embeddings:\n",
        "    cosine_similarities.append(cosine_similarity(embedding_pair[0].unsqueeze(0), embedding_pair[1].unsqueeze(0)).item())\n",
        "\n",
        "correlation_coefficient = val_df['score'].corr(pd.Series(cosine_similarities))\n",
        "print(\"Correlation coefficient (Pearson correlation) between predicted similarities and actual scores:\", correlation_coefficient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpwXBjnrZebc"
      },
      "source": [
        "## Setup 1C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaFWJeXfbp9P"
      },
      "source": [
        "In this setup, you must fine-tune the Sentence-BERT model for the task of\n",
        "STS. Make use of the CosineSimilarityLoss function (Losses — Sentence-Transformers\n",
        "documentation). Report the required evaluation metric on the validation set—reference:\n",
        "Semantic Textual Similarity — Sentence-Transformers documentation. You must train for\n",
        "at least two epochs and surpass the performance of Setup 2B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yIidmx_WEIj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5lUdEKZWEF_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6KkQgKzWEC3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DOcCz-OWEAb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51avZrT9WD-t"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJEBJg7EWD5c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1rM4DplWA1j"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
