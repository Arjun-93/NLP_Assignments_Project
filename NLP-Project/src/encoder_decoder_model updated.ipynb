{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoders:\n",
    " - https://huggingface.co/monsoon-nlp/hindi-bert\n",
    " - https://huggingface.co/flax-community/roberta-hindi\n",
    " - https://huggingface.co/vasista22/whisper-hindi-small\n",
    " \n",
    "Decoders:\n",
    "- https://huggingface.co/csebuetnlp/mT5_m2o_hindi_crossSum\n",
    "- https://huggingface.co/docs/transformers/model_doc/openai-gpt\n",
    "- https://huggingface.co/docs/transformers/model_doc/ctrl\n",
    "- https://huggingface.co/docs/transformers/model_doc/gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import EncoderDecoderModel, AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"monsoon-nlp/hindi-bert\")\n",
    "# model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"monsoon-nlp/hindi-bert\", \"gpt2\")\n",
    "\n",
    "# model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# # a hindi sentence\n",
    "# input_ids = tokenizer(\n",
    "#     'भारतीय राष्ट्रीय कांग्रेस के अध्यक्ष राहुल गांधी ने शनिवार को कहा कि भारतीय जनता पार्टी ने देश को एक दशक में वापस ले जाया है।',\n",
    "#     return_tensors=\"pt\",\n",
    "# ).input_ids\n",
    "\n",
    "# labels = tokenizer(\n",
    "#     'राहुल गांधी ने कहा कि भारतीय जनता पार्टी ने देश को एक दशक में वापस ले जाया है।', return_tensors=\"pt\",\n",
    "# ).input_ids\n",
    "\n",
    "# # the forward function automatically creates the correct decoder_input_ids\n",
    "# outputs = model(input_ids=input_ids, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = model.generate(input_ids, decoder_start_token_id=model.config.decoder_start_token_id)\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer2.decode(outputs[0], skip_special_tokens=True, predict_with_generate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21225, 4) (3000, 3)\n",
      "(18041, 4) (3184, 4) (3000, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_df = pd.read_csv('/raid/home/dhruv/pankaji/NLP_PROJECT/Data/HindiNews_test.txt')\n",
    "train_df = pd.read_csv('/raid/home/dhruv/pankaji/NLP_PROJECT/Data/hindi_train.txt')\n",
    "print(train_df.shape, test_df.shape)\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "test_df = test_df.dropna()\n",
    "\n",
    "train_val_split = 0.15\n",
    "train_df, val_df = train_test_split(train_df, test_size=train_val_split, random_state=42)\n",
    "print(train_df.shape, val_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Heading</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8713</th>\n",
       "      <td>hindi_2023_train_8713</td>\n",
       "      <td>प्रोडक्शन के लिए एम्ब्रेयर और सुखोई की बातचीत ...</td>\n",
       "      <td>india looks partner sukhoi embraer to manufact...</td>\n",
       "      <td>भारत सरकार दूर-दराज इलाकों से बेहतर एयर कनेक्ट...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19043</th>\n",
       "      <td>hindi_2022_9186</td>\n",
       "      <td>UP: भदोही में नशे में धुत युवक ने महात्मा गांध...</td>\n",
       "      <td>जिसके बाद कांग्रेस कार्यकर्ता एकत्रित होकर गां...</td>\n",
       "      <td>भदोहीः उत्तर प्रदेश के भदोही में बीती शनिवार र...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17257</th>\n",
       "      <td>hindi_2022_7400</td>\n",
       "      <td>भारत के इस क्षेत्र में नहीं है Coronavirus का ...</td>\n",
       "      <td>भारत में कोविड-19 के मामले 97.35 लाख के पार चल...</td>\n",
       "      <td>कोच्चि: भारत में कोविड-19 के मामले 97.35 लाख क...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3156</th>\n",
       "      <td>hindi_2023_train_3156</td>\n",
       "      <td>12वीं में पासिंग परसेंटेज के आधार पर परीक्षा म...</td>\n",
       "      <td>NEET 2024 - No Minimum Marks Needed For NEET. ...</td>\n",
       "      <td>12वीं के बाद मेडिकल की पढ़ाई करने वाले छात्रों ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15583</th>\n",
       "      <td>hindi_2022_5726</td>\n",
       "      <td>उत्तर प्रदेश में कोरोना वायरस से 17 और मौतें, ...</td>\n",
       "      <td>उत्‍तर प्रदेश में शुक्रवार को कोरोना वायरस संक...</td>\n",
       "      <td>लखनऊ: उत्‍तर प्रदेश में शुक्रवार को कोरोना वाय...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Id  \\\n",
       "8713   hindi_2023_train_8713   \n",
       "19043        hindi_2022_9186   \n",
       "17257        hindi_2022_7400   \n",
       "3156   hindi_2023_train_3156   \n",
       "15583        hindi_2022_5726   \n",
       "\n",
       "                                                 Heading  \\\n",
       "8713   प्रोडक्शन के लिए एम्ब्रेयर और सुखोई की बातचीत ...   \n",
       "19043  UP: भदोही में नशे में धुत युवक ने महात्मा गांध...   \n",
       "17257  भारत के इस क्षेत्र में नहीं है Coronavirus का ...   \n",
       "3156   12वीं में पासिंग परसेंटेज के आधार पर परीक्षा म...   \n",
       "15583  उत्तर प्रदेश में कोरोना वायरस से 17 और मौतें, ...   \n",
       "\n",
       "                                                 Summary  \\\n",
       "8713   india looks partner sukhoi embraer to manufact...   \n",
       "19043  जिसके बाद कांग्रेस कार्यकर्ता एकत्रित होकर गां...   \n",
       "17257  भारत में कोविड-19 के मामले 97.35 लाख के पार चल...   \n",
       "3156   NEET 2024 - No Minimum Marks Needed For NEET. ...   \n",
       "15583  उत्‍तर प्रदेश में शुक्रवार को कोरोना वायरस संक...   \n",
       "\n",
       "                                                 Article  \n",
       "8713   भारत सरकार दूर-दराज इलाकों से बेहतर एयर कनेक्ट...  \n",
       "19043  भदोहीः उत्तर प्रदेश के भदोही में बीती शनिवार र...  \n",
       "17257  कोच्चि: भारत में कोविड-19 के मामले 97.35 लाख क...  \n",
       "3156   12वीं के बाद मेडिकल की पढ़ाई करने वाले छात्रों ...  \n",
       "15583  लखनऊ: उत्‍तर प्रदेश में शुक्रवार को कोरोना वाय...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/raid/home/dhruv/miniconda3/envs/myenv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import EncoderDecoderModel, AutoTokenizer, AutoModelForCausalLM,AutoModelForSeq2SeqLM, MT5ForConditionalGeneration\n",
    "\n",
    "tokenizer_enc = AutoTokenizer.from_pretrained(\"csebuetnlp/mT5_m2o_hindi_crossSum\") #\"aashay96/indic-gpt\"\n",
    "#model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"ai4bharat/IndicBART\",\"facebook/mbart-large-en-ro\" )\n",
    "tokenizer_dec = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50\")\n",
    "\n",
    "\n",
    "#decoder_model.enc_to_dec_proj=torch.nn.Linear(768, len(tokenizer))\n",
    "#decoder_model.enc_to_dec_proj.name = \"lm_head\"\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder_model =  AutoModelForSeq2SeqLM.from_pretrained(\"csebuetnlp/mT5_m2o_hindi_crossSum\").get_encoder()\n",
    "#model = EncoderDecoderModel.from_encoder_decoder_pretrained(encoder_model=encoder_model, decoder_pretrained_model_name_or_path=\"facebook/mbart-large-50\")\n",
    "model_decoder = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-50\").get_decoder()\n",
    "model_decoder.config.decoder_start_token_id = tokenizer_dec.pad_token_id#tokenizer.cls_token_id\n",
    "encoder_model.config.pad_token_id = tokenizer_enc.pad_token_id\n",
    "tokenizer_enc.add_tokens([\"<sum>\"], special_tokens=True )\n",
    "tokenizer_enc.add_tokens([\"<sum>\"], special_tokens=True )\n",
    "encoder_model = encoder_model.to(device)\n",
    "model_decoder = model_decoder.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBartForConditionalGeneration(\n",
       "  (model): MBartModel(\n",
       "    (shared): Embedding(250054, 1024, padding_idx=1)\n",
       "    (encoder): MBartEncoder(\n",
       "      (embed_tokens): Embedding(250054, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): MBartDecoder(\n",
       "      (embed_tokens): Embedding(250054, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBartDecoder(\n",
       "  (embed_tokens): Embedding(250054, 1024, padding_idx=1)\n",
       "  (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x MBartDecoderLayer(\n",
       "      (self_attn): MBartAttention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (activation_fn): GELUActivation()\n",
       "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder_attn): MBartAttention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MT5Stack(\n",
       "  (embed_tokens): Embedding(250112, 768)\n",
       "  (block): ModuleList(\n",
       "    (0): MT5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): MT5LayerSelfAttention(\n",
       "          (SelfAttention): MT5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (relative_attention_bias): Embedding(32, 12)\n",
       "          )\n",
       "          (layer_norm): MT5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): MT5LayerFF(\n",
       "          (DenseReluDense): MT5DenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): MT5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1-11): 11 x MT5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): MT5LayerSelfAttention(\n",
       "          (SelfAttention): MT5Attention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): MT5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): MT5LayerFF(\n",
       "          (DenseReluDense): MT5DenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): MT5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): MT5LayerNorm()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train = iter(train_df['Heading'].iloc[i]+train_df['Article'].iloc[i] for i in range(len(train_df)))\n",
    "# new_tokenizer = tokenizer.train_new_from_iterator(train, vocab_size=10000)\n",
    "# new_tokens = set(new_tokenizer.vocab) - set(tokenizer.vocab)\n",
    "# len(new_tokenizer.vocab)\n",
    "# tokenizer.add_tokens(list(new_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(250101, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_decoder.resize_token_embeddings(len(tokenizer_dec))\n",
    "#model.resize_token_embeddings(len(tokenizer))\n",
    "encoder_model.resize_token_embeddings(len(tokenizer_enc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class m(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(m,self).__init__()\n",
    "        self.device = device\n",
    "        self.linear = nn.Linear(768, 1024, device = self.device)\n",
    "        self.encoder_model = encoder_model\n",
    "        self.decoder_model = model_decoder\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.lm_head = nn.Linear(1024,len(tokenizer_dec), device = self.device)\n",
    "    def forward(self, input_ids, attention_mask, labels_input_ids, labels_attention_mask):\n",
    "        out_enc = self.encoder_model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        enc_hidden_state = out_enc.last_hidden_state\n",
    "        enc_hidden_state = self.linear(enc_hidden_state)\n",
    "        out_dec = self.decoder_model(input_ids = labels_input_ids, attention_mask = labels_attention_mask, encoder_hidden_states=enc_hidden_state) \n",
    "        logits = self.lm_head(out_dec.last_hidden_state)\n",
    "        loss = self.loss(logits.squeeze(), labels_input_ids.squeeze())\n",
    "        return logits ,loss           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = m()\n",
    "optimizer = optim.Adam(mod.parameters(), lr=1e-3)\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Batch 1000 Loss: 1.5505152671933173\n",
      "Batch 2000 Loss: 1.136932954502292\n",
      "Batch 3000 Loss: 1.2455086765419692\n",
      "Batch 4000 Loss: 0.9942877303548158\n",
      "Batch 5000 Loss: 0.9375281589161605\n",
      "Batch 6000 Loss: 0.7536206559808925\n",
      "Batch 7000 Loss: 0.71439530245075\n",
      "Batch 8000 Loss: 0.6416664533203003\n",
      "Batch 9000 Loss: 0.6225625697569922\n",
      "Batch 10000 Loss: 0.543062920810422\n",
      "Batch 11000 Loss: 0.5361014707612339\n",
      "Batch 12000 Loss: 0.5252809669609997\n",
      "Batch 13000 Loss: 0.5133743769332068\n",
      "Batch 14000 Loss: 0.4544009827639093\n",
      "Batch 15000 Loss: 0.4437603746164823\n",
      "Batch 16000 Loss: 0.41798241723170215\n",
      "Batch 17000 Loss: 0.3919906807463267\n",
      "Batch 18000 Loss: 0.41697318682889456\n",
      "Training Loss: 0.7128633632289189\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "evaluate() missing 1 required positional argument: 'val_df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m train(mod, tokenizer_enc, tokenizer_dec, train_df, optimizer)\n\u001b[0;32m---> 49\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_dec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: evaluate() missing 1 required positional argument: 'val_df'"
     ]
    }
   ],
   "source": [
    "def train(model, tokenizer_enc, tokenizer_dec, train_df, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch = 0\n",
    "    batch_loss = 0\n",
    "    batch_size = 1000\n",
    "    for i, row in train_df.iterrows():\n",
    "      \n",
    "        input = tokenizer_enc(\"<sum>\"+row[\"Article\"], return_tensors=\"pt\",max_length=1024, truncation=True)\n",
    "        input_ids = input.input_ids.to(device)\n",
    "        attention_mask = input.attention_mask.to(device)\n",
    "        labels = tokenizer_dec(row['Summary'], return_tensors=\"pt\")\n",
    "        labels_input_ids = labels.input_ids.to(device)\n",
    "        labels_attention_mask = labels.attention_mask.to(device)\n",
    "        \n",
    "        logits, loss = model(input_ids=input_ids, attention_mask=attention_mask,  labels_input_ids=labels_input_ids,labels_attention_mask=labels_attention_mask)\n",
    "        #outputs = model(input_ids=input_ids, attention_mask=attention_mask,  labels=labels_input_ids, decoder_attention_mask=labels_attention_mask) \n",
    "        #loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        batch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        batch += 1\n",
    "        if batch % batch_size == 0:\n",
    "            print(f\"Batch {batch} Loss: {batch_loss / batch_size}\")\n",
    "            batch_loss = 0\n",
    "    print(f\"Training Loss: {total_loss / len(train_df)}\")\n",
    "        \n",
    "\n",
    "def evaluate(model, tokenizer_enc, tokenizer_dec, val_df):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for i, row in val_df.iterrows():\n",
    "        input = tokenizer_enc(\"<sum>\"+row[\"Article\"], return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        input_ids = input.input_ids.to(device)\n",
    "        attention_mask = input.attention_mask.to(device)\n",
    "        labels = tokenizer_dec(row['Summary'], return_tensors=\"pt\")\n",
    "        labels_input_ids = labels.input_ids.to(device)\n",
    "        labels_attention_mask = labels.attention_mask.to(device)\n",
    "        #outputs = model(input_ids=input_ids, attention_mask=attention_mask,  labels=labels_input_ids, decoder_attention_mask=labels_attention_mask)\n",
    "        logits, loss = model(input_ids=input_ids, attention_mask=attention_mask,  labels_input_ids=labels_input_ids,labels_attention_mask=labels_attention_mask)\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Validation Loss: {total_loss / len(val_df)}\")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    train(mod, tokenizer_enc, tokenizer_dec, train_df, optimizer)\n",
    "    evaluate(mod, tokenizer_enc, tokenizer_dec, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HindiNews_test_6\n",
      "राष्ट्रपति द्रौपदी मुर्मू गुरुवार को असम जाएंगीं। वे 6 और 7 अप्रैल को इस राज्य के दौरे पर रहेंगीं। इस दौरान राष्ट्रपति यहां काजीरंगा नेशनल पार्क में एलिफेंट महोत्सव में भाग लेंगीं। मीडिया रिपोर्ट्स के मुताबिक राष्ट्रपति की यात्रा के दौरान 6 और 7 अप्रैल को काजीरंगा राष्ट्रीय उद्यान की दो रेंज पर्यटकों के लिए बंद रहेंगी।\n",
      "हाथी प्रोजेक्ट के 30 साल पूरे होने का जश्न मनाया जा रहा\n",
      "यह उत्सव राष्ट्रीय उद्यान में हर साल आयोजित किया जाता है। इस साल केंद्र और राज्य सरकार के पर्यावरण और वन विभाग संयुक्त रूप से इसका आयोजन कर रहे हैं। यह महोत्सव 7 और 8 अप्रैल को होगा। महोत्सव का आयोजन हाथी प्रोजेक्ट के 30 साल पूरे होने का जश्न मनाने और संरक्षण प्रयासों को बढ़ावा देने के लिए किया जा रहा है।\n",
      "पर्यटकों के लिए बंद रहेगी जीप और हाथी सफारी\n",
      "कार्यक्रम के दौरान पर्यटकों के लिए उद्यान में जीप और हाथी सफारी दो दिनों तक बंद रहेगी। काजीरंगा नेशनल पार्क असम के गोलाघाट और नागांव क्षेत्र में स्थित है। यह असम का सबसे पुराना उद्यान है जो उत्तर में ब्रह्मपुत्र नदी के किनारे और दक्षिण में कार्बी आंगलोंग पहाड़ियों के पास 430 वर्ग KM में फैला हुआ है।\n",
      "यह राष्ट्रीय उद्यान एक सींग वाले गैंडों के लिए दुनियाभर में फेमस है। यही वजह है कि यूनेस्को ने इस नेशनल पार्क को यूनेस्को की विश्व धरोहर सूची में शामिल किया है।\n",
      "ये खबरें भी पढ़ सकते हैं...\n",
      "राष्ट्रपति का देश के नाम संदेश, मुर्मू बोलीं- भारत गरीब और निरक्षर राष्ट्र से बढ़कर विश्व मंच पर आत्मविश्वास से भरा देश बना\n",
      "गणतंत्र दिवस की पूर्व संध्या पर राष्ट्रपति द्रौपदी मुर्मु ने राष्ट्र के नाम अपने संदेश में कहा था- 74वें गणतंत्र दिवस की पूर्व संध्या पर, देश और विदेश में रहने वाले आप सभी भारत के लोगों को, मैं हार्दिक बधाई देती हूं। जब हम गणतंत्र दिवस मनाते हैं, तब एक राष्ट्र के रूप में हमने मिल-जुलकर जो उपलब्धियां प्राप्त की हैं, उनका हम उत्सव मनाते हैं। पूरी खबर यहां पढ़ें...\n",
      "राष्ट्रपति द्रौपदी मुर्मू ने काफिला रुकवाकर बच्चों को चॉकलेट बांटी, कुछ देर बातचीत की और गुड बाय बोलकर चली गईं\n",
      "राष्ट्रपति द्रौपदी मुर्मू ने केरल के कोल्लम जिले में अपना काफिला रुकवाकर सड़क किनारे खड़े स्कूली बच्चों को चॉकलेट बांटी थी। उसके बाद उन्होंने कुछ देर बच्चों से बात की और गुड बाय बोलकर चली गईं। बच्चों ने खुश होकर तालियां बजाईं और राष्ट्रपति मुर्मू को एक साथ जोर से थैंक्यू बोला। पूरी खबर यहां पढ़ें...\n",
      "torch.Size([1, 823])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'m' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer_enc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum>\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtext, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_ids\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m(input_ids, decoder_start_token_id\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id, num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# tokenizer2\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'm' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "head_text = test_df['Heading'][18]\n",
    "text = test_df[\"Article\"][19]\n",
    "print(test_df['id'][6])\n",
    "print(text)\n",
    "\n",
    "input_ids = tokenizer_enc(\"sum>\"+text, return_tensors=\"pt\", truncation=True).input_ids.to(device)\n",
    "print(input_ids.shape)\n",
    "\n",
    "outputs = mod.generate(input_ids, decoder_start_token_id=model.config.decoder_start_token_id, num_beams=4)\n",
    "print(outputs)\n",
    "# tokenizer2\n",
    "summary = tokenizer_dec.decode(outputs[0], skip_special_tokens=True, predict_with_generate=True)\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'Article', 'Heading'], dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"Article\"].iloc[3]\n",
    "test_df[\"Heading\"].iloc[3]\n",
    "test_df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
